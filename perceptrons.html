<!doctype html>

<html>

<head>

    <title>Neural Networks and their applications</title>
    <link rel="stylesheet" type="text/css" href="css/main.css" />

</head>

<body>

    <div id="wrapper">

        <a href="index.html" id="header"></a> 

        <div id="navigation">
<script type='text/javascript' src='https://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js'></script>
<script type='text/javascript'>
$(function() {
    // Stick the #nav to the top of the window
    var nav = $('#navigation');
    var navHomeY = nav.offset().top;
    var isFixed = false;
    var $w = $(window);
    $w.scroll(function() {
        var scrollTop = $w.scrollTop();
        var shouldBeFixed = scrollTop > navHomeY;
        if (shouldBeFixed && !isFixed) {
            nav.css({
                position: 'fixed',
                top: 0,
                left:  nav.offset().left,
                width: nav.width()
            });
            isFixed = true;
        }
        else if (!shouldBeFixed && isFixed)
        {
            nav.css({
                padding: 9,
                left: 0,
                position: 'relative'
           
            });
            isFixed = false;
        }
    });
});
</script>

            <ul id="nav-one" class="nav">

                <li>
                    <a href="introduction.html">Introduction</a>
                </li>

                <li>
                    <a href="perceptrons.html">Perceptrons</a>
                </li>

                <li>
                    <a href="backpropagation.html">Backpropagation</a>
                </li>

                <li>
                    <a href="applet.html">Applet</a>
                </li>
                
            </ul>
            
        </div>
 
        <div id="faux">
        
            <div id="leftcolumn">
		    </div>
		    
		    <div id="content">
                  
                <h1>History</h1>
                
                <p>
                    Perceptrons where originally investigated by Frank Rosenblatt in 1957 at the
                    Cornell Aeronautical Laboratory at Cornell University. This was an attempt to
                    understand human memory, learning and cognitive processes. His first attempt
                    at this was the Mark I Perceptron, which could learn to recognise and identify
                    optical patterns. Rosenblatt's work was based upon and also advanced the work
                    by Warren McCulloch and Walter Pitts. McCulloch and Pitts were the first to
                    consider the concept of neural networks. They developed the MP neuron based upon
                    the concept of a human brain - the nerve firing an impulse to another nerve only
                    if a threshold value is exceeded. The MP neuron didn't have the ability to learn,
                    therefore Rosenblatt tried to solve this problem by implementing Donald Hebb's
                    idea that "when an axon of cell A is near enough to excite a cell B and repeatedly
                    or persistently takes part in firing it, some growth process or metabolic change
                    takes place in one or both cells, such that A's efficiency, as one of the cells
                    firing B, is increased". This implied the idea of a network of neurons which could
                    learn by adjusting the weights on connections between neurons. This is the idea
                    which a perceptron works upon.
                </p>

                <h1>Single Layer Perceptrons</h1>
                               
                <p>
                    So named because there is an input layer and an output layer, where each
                    layer is fully connected to the other layer. There are no connections within
                    the layers, just like in a complete bipartite graph. A perceptron is the
                    simplest kind of neural network, and is also a feedforward neural network,
                    meaning that there is no directed cycles. In this network, information only
                    moves in one direction, straight from the input layer to the output layer.
                </p>
                
                <p>
                    To calculate the output of the network, each input is put onto the
                    corresponding input neuron. Each input neuron then passes this value unmodified
                    to the output layer through the synapses, which multiply the input by the weight
                    of the synapse. The value of an output neuron is the sum of every input the
                    neuron recieves from the synapses leading to it. Once all these calculations are
                    made, the output of the network is simply the value of the output neurons.
                </p>
                
                <div id="img">
                    
                    <img src="images/singleLayerPerceptron.png" title="Single layer perceptron diagram"/>
                    <p>Image source: created by Jaime Lennox</p>
                    
                                    
                </div>
			       
                <h2>Limitations</h2>

                <p>
                    Perceptron networks have limitations though. Firstly, the output value of the
                    perceptron can only be either true or false, meaning that it can only say for
                    definite whether the input is what it is, and not how close it is. Also, a
                    perceptron can only recognize linearly separable patterns. This means that
                    any input pattern can be separated into two distinct classes by drawing a
                    single line, as demonstrated below for AND/OR.
                </p>

                <div id="img">

                  <img src="images/linearlySeparableProblem1.png" title="Linearly separable problem diagram 1"/>
                  <p>Image source: http://itee.uq.edu.au/~cogs2010/cmc/chapters/BackProp/index2.html</p>

                </div>

                <p>                    
                    A problem arises when this same method is attempted on XOR -
                    it isn't linearly separable:
                </p>

                <div id="img">

                  <img src="images/linearlySeparableProblem2.png" title="Linearly separable problem diagram 2" />
                  <p>Image source: http://itee.uq.edu.au/~cogs2010/cmc/chapters/BackProp/index2.html</p>

                </div>


                <p>
                    A way of fixing this is to get it to be seperable on a 3-D
                    plane - on the diagram above, you can imagine the 1s being
                    moved up and the 0s being moved down so a line can be drawn
                    inbetween. This is effectively done when using multi layer
                    perceptrons.
                
                <h1>Multi Layer Perceptrons</h1>
                
                <p>
                    Multi-layer perceptrons are very similar to single layer perceptrons, with the exception of one or more hidden
                    layers between the input and output layers. These work in the same way, with each neuron in the previous layer
                    being connected to every neuron in the next layer.
                </p>
                
                <p>
                    The value of a neuron in a hidden layer is calculated like the value of an output neuron: the sum of the output
                    of every synapse leading to the neuron. However, a special type of function called the activation function
                    is then applied to this sum to calculate the final value of the hidden neuron. The activation function is a
                    non-linear differentiable function which tends to 1 as the input tends to positive infinity and -1
                    as the input tends to negative infinity. Applying this function "squashes" the output of every hidden neuron
                    between 1 and -1. The logistics function, <em>1 / (1 + exp(-x))</em>, is commonly used as the activation
                    function of a network.
                </p>
                
                <p>
                    The activation function is needed to gain an advantage from using hidden layers, without the activation function
                    any multi-layer perceptron can be reduced to a single layer percetron with equivalent output. The input and
                    output neurons behave in the same way as in a single layer perceptron.
                </p>

                <div id="img">

                    <img src="images/multiLayerPerceptron.png" title="Multilayer Perceptron diagram"/>
                    <p>Image source: created by Jaime Lennox</p>

                </div>

                <p>
                    The advantage of the extra neurons means it can be used to learn more complex patterns, since each layer can
                    find patterns in the previous layer, which will itself have found patterns in the layer before it and so on,
                    meaning that the network has more 'depth'. However, adding too many layers for a simple problem can make a
                    network learn much too slowly, or even make it start to see patterns that are irrelavent.
                </p>
                
                <h1>References</h1>
                
                <div id="ref">

                    <ol>

                        <li>
                           <a href="http://www.csulb.edu/~cwallis/artificialn/History.htm">
                             http://www.csulb.edu/~cwallis/artificialn/History.htm</a>
                        </li>                  
                        
                        <li>
                            <a href="http://ei.cs.vt.edu/~history/Perceptrons.Estebon.html">
                              http://ei.cs.vt.edu/~history/Perceptrons.Estebon.html</a>
                        </li>

                        <li>
                            <a
                              href="http://itee.uq.edu.au/~cogs2010/cmc/chapters/BackProp/index2.html">
                             http://itee.uq.edu.au/~cogs2010/cmc/chapters/BackProp/index2.html</a>
                        </li>
                    
                        <li>
                            A. K. Dewdney (1993). <em>The New Turning Omibus.</em> New York: W. H. Freeman and Company. p241-248.
                        </li>

                    </ol>
                
                </div>
		       			  
		    </div>
		    
		    <div id="rightcolumn">
		    </div>

        </div>	   

        <div id="footer">
		      
            Imperial College Computing		

        </div>
		 
    </div>
    
</body>

</html>
