<!doctype html>

<html>

<head>

    <title>Neural Networks and their applications</title>
    <link rel="stylesheet" type="text/css" href="css/main.css" />

</head>

<body>

    <div id="wrapper">

        <a href="index.html" id="header"></a> 

        <div id="navigation">
<script type='text/javascript' src='https://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js'></script>
<script type='text/javascript'>
$(function() {
    // Stick the #nav to the top of the window
    var nav = $('#navigation');
    var navHomeY = nav.offset().top;
    var isFixed = false;
    var $w = $(window);
    $w.scroll(function() {
        var scrollTop = $w.scrollTop();
        var shouldBeFixed = scrollTop > navHomeY;
        if (shouldBeFixed && !isFixed) {
            nav.css({
                position: 'fixed',
                top: 0,
                left:  nav.offset().left,
                width: nav.width()
            });
            isFixed = true;
        }
        else if (!shouldBeFixed && isFixed)
        {
            nav.css({
                padding: 9,
                left: 0,
                position: 'relative'
           
            });
            isFixed = false;
        }
    });
});
</script>

            <ul id="nav-one" class="nav">

                <li>
                    <a href="introduction.html">Introduction</a>
                </li>

                <li>
                    <a href="perceptrons.html">Perceptrons</a>
                </li>

                <li>
                    <a href="backpropagation.html">Backpropagation</a>
                </li>

                <li>
                    <a href="applet.html">Applet</a>
                </li>
                
            </ul>
            
        </div>
 
        <div id="faux">
        
            <div id="leftcolumn">
		    </div>
		    
		    <div id="content">
                  
                <h1>History</h1>
                
                <p>
                    Perceptrons where originally investigated by Frank Rosenblatt in 1957 at the
                    Cornell Aeronautical Laboratory at Cornell University. This was an attempt to
                    understand human memory, learning and cognitive processes. His first attempt
                    at this was the Mark I Perceptron, which could learn to recognise and identify
                    optical patterns. Rosenblatt's work was based upon and also advanced the work
                    by Warren McCulloch and Walter Pitts. McCulloch and Pitts were the first to
                    consider the concept of neural networks. They developed the MP neuron based upon
                    the concept of a human brain - the nerve firing an impulse to another nerve only
                    if a threshold value is exceeded. The MP neuron didn't have the ability to learn,
                    therefore Rosenblatt tried to solve this problem by implementing Donald Hebb's
                    idea that "when an axon of cell A is near enough to excite a cell B and repeatedly
                    or persistently takes part in firing it, some growth process or metabolic change
                    takes place in one or both cells, such that A's efficiency, as one of the cells
                    firing B, is increased". This implied the idea of a network of neurons which could
                    learn by adjusting the weights on connections between neurons. This is the idea
                    which a perceptron works upon.
                </p>

                <h1>Single Layer Perceptrons</h1>
                               
                <p>
                    So named because there is an input layer and an output layer, where each
                    layer is fully connected to the other layer. There are no connections within
                    the layers, just like in a complete bipartite graph. A perceptron is the
                    simplest kind of neural network, and is also a feedforward neural network,
                    meaning that there is no directed cycles. In this network, information only
                    moves in one direction, straight from the input layer to the output layer.
                    The perceptron works by the input layer sending a signal to the output
                    player through the weighted connections. Each node on the output layer sums
                    up the incoming signal values, and if a threshold is exceeded, the node
                    fires an output signal.
                </p>
                
                <div id="img">
                    
                    <img src="images/singleLayerPerceptron.png" title="Single layer perceptron diagram"/>
                    <p>Image source: created by Jaime Lennox</p>
                    
                                    
                </div>
			       
                <h2>Limitations</h2>

                <p>
                    Perceptron networks have limitations though. Firstly, the output value of the
                    perceptron can only be either true or false, meaning that it can only say for
                    definite whether the input is what it is, and not how close it is. Also, a
                    perceptron can only recognize linearly separable patterns. This means that
                    any input pattern can be separated into two distinct classes by drawing a
                    single line, as demonstrated below for AND/OR.
                </p>

                <div id="img">

                  <img src="images/linearlySeparableProblem1.png" title="Linearly separable problem diagram 1"/>
                  <p>Image source: http://itee.uq.edu.au/~cogs2010/cmc/chapters/BackProp/index2.html</p>

                </div>

                <p>                    
                    A problem arises when this same method is attempted on XOR -
                    it isn't linearly separable:
                </p>

                <div id="img">

                  <img src="images/linearlySeparableProblem2.png" title="Linearly separable problem diagram 2" />
                  <p>Image source: http://itee.uq.edu.au/~cogs2010/cmc/chapters/BackProp/index2.html</p>

                </div>


                <p>
                    A way of fixing this is to get it to be seperable on a 3-D
                    plane - on the diagram above, you can imagine the 1s being
                    moved up and the 0s being moved down so a line can be drawn
                    inbetween. This is effectively done when using multi layer
                    perceptrons.
                
                <h1>Multi Layer Perceptrons</h1>
                
                <p>
                    Multi-layer perceptrons are very similar to single layer perceptrons, with the exception of some hidden layers
                    between the input and output layers. These work in the same way, with each neuron in the previous layer being
                    connected to every neuron in the next layer.
                </p>

                <div id="img">

                    <img src="images/multiLayerPerceptron.png" title="Multilayer Perceptron diagram"/>
                    <p>Image source: created by Jaime Lennox</p>

                </div>

                <p>
                    The advantage of the extra neurons means it can be used to learn more complex patterns, since each layer can
                    find patterns in the previous layer, which will itself have found patterns in the layer before it and so on,
                    meaning that the network has more 'depth'. However, adding too many layers for a simple problem can make a
                    network learn much too slowly, or even make it start to see patterns that are irrelavent.
                </p>
                
                <h1>References</h1>
                
                <div id="ref">

                    <ol>

                        <li>
                           <a href="http://www.csulb.edu/~cwallis/artificialn/History.htm">
                             http://www.csulb.edu/~cwallis/artificialn/History.htm</a>
                        </li>                  
                        
                        <li>
                            <a href="http://ei.cs.vt.edu/~history/Perceptrons.Estebon.html">
                              http://ei.cs.vt.edu/~history/Perceptrons.Estebon.html</a>
                        </li>

                        <li>
                            <a
                              href="http://itee.uq.edu.au/~cogs2010/cmc/chapters/BackProp/index2.html">
                             http://itee.uq.edu.au/~cogs2010/cmc/chapters/BackProp/index2.html</a>
                        </li>
                        
                        <li>
                            <a href="http://en.wikipedia.org/wiki/Feedforward_neural_network">
                              http://en.wikipedia.org/wiki/Feedforward_neural_network</a>
                        </li>

                    </ol>
                
                </div>
		       			  
		    </div>
		    
		    <div id="rightcolumn">
		    </div>

        </div>	   

        <div id="footer">
		      
            Imperial College Computing		

        </div>
		 
    </div>
    
</body>

</html>
